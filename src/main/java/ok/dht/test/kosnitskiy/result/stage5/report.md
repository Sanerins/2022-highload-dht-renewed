ENVIRONMENT-DESCRIPTION
Тестирование производилось на 6-ядерном, 6-поточном процессоре. 
Были запущены 3 ноды, дожидались ответа от двух нод, реплецировали на все три

GET-DESCRIPTION
Для тестирования было решено использовать ключи, закрытые сверху еще 1.5 гигами информации с другими ключами. 
Это стандартный юзкейс для нашей базы данных - ключи существуют, однако чтобы их достать прийдется порыться по файлам
и выкопать их их глубины

PUT-DESCRIPTION
Для PUT мы генерировали разные ключи, чтобы гарантировать, что размер DAO будет увеличиваться и не будет
перезаписываться одно и тоже значение кучу раз.
Так же мы стали генерировать строки разной длинны чтобы несколько усложить бинарный поиск по получившимся файлам

WRK-PUT
◆ utils git:(stage5) ✗ ❯❯❯ wrk -d 20s -t 4 -c 64 -R 2500 -s put-different.lua --latency "http://localhost:19230"
Running 20s test @ http://localhost:19230
4 threads and 64 connections
Thread calibration: mean lat.: 7.354ms, rate sampling interval: 32ms
Thread calibration: mean lat.: 6.966ms, rate sampling interval: 30ms
Thread calibration: mean lat.: 7.515ms, rate sampling interval: 34ms
Thread calibration: mean lat.: 7.400ms, rate sampling interval: 33ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency    10.58ms   12.82ms  98.75ms   87.55%
Req/Sec   636.60    213.31     1.50k    75.00%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%    5.72ms
75.000%   12.98ms
90.000%   27.06ms
99.000%   62.17ms
99.900%   79.55ms
99.990%   86.33ms
99.999%   98.82ms
100.000%   98.82ms

Тестирование производилось при нагрузке node 0
Легко заметить, что наша скорость добавления данных почти не изменилось при использовании future.
Это связано с тем, что put - очень быстрая операция для нашей базы данных, а потому затраты на использование
future здесь примерно выходят вровень с преимуществом асинхронного способа отправки запросов



WRK-GET
◆ utils git:(stage5) ✗ ❯❯❯ wrk -d 20s -t 4 -c 64 -R 2500 -s get-random.lua --latency "http://localhost:19230"
Running 20s test @ http://localhost:19230
4 threads and 64 connections
Thread calibration: mean lat.: 13.951ms, rate sampling interval: 73ms
Thread calibration: mean lat.: 13.339ms, rate sampling interval: 70ms
Thread calibration: mean lat.: 13.551ms, rate sampling interval: 71ms
Thread calibration: mean lat.: 13.622ms, rate sampling interval: 74ms
Thread Stats   Avg      Stdev     Max   +/- Stdev
Latency    14.82ms   24.92ms 192.00ms   93.26%
Req/Sec   630.28    137.60     1.21k    77.19%
Latency Distribution (HdrHistogram - Recorded Latency)
50.000%    6.63ms
75.000%   16.03ms
90.000%   31.22ms
99.000%  145.92ms
99.900%  172.80ms
99.990%  187.01ms
99.999%  192.13ms
100.000%  192.13ms


С другой стороны скорость куда более долгих гетов сильно возросла по сравнению с прошлым разом. Учитывая, что операции
получения данных занимают куда больше времени, здесь возможность обрабатывать запросы асинхронно не ждя ответа очень
сильно повышает нашу производительность и возможности

Мы смогли негилировать часть проблем, которые я обозничил в четвертом стейдже, и привести производительность к более 
достойному уровню

По флеймграфам видим, что по сравнению с прошлым стейджем количество блокировок стало значительно меньше, так же
нагрузка на процессор стало распределена куда более равномерно - результат того, что теперь мы не простаиваем, 
ожидая ответа. Это проявляется тем, что sendAsync почти невидим на графиках, в отличие от send в прошлом стейдже.
Однако получение ответа через AsyncReceiver все же занимает порядочное место на наших графиках. Тут наверное остается
место для еще одной оптимизации - когда нам приходит много запросов, возможно быстрее будет собирать их в пачку и
отправлять другим нодам сразу охапкой, ожижая охапку в ответ. Так же использование более быстрого, чем HTTP, протокола
тоже может в теории сильно улучшить ситуацию

Спасибо за прочтение!